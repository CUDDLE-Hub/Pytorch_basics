{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "grave-charge",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install torch\n",
    "!{sys.executable} -m pip install torchvision\n",
    "!{sys.executable} -m pip install matplotlib\n",
    "!{sys.executable} -m pip install tqdm\n",
    "!{sys.executable} -m pip install scikit-learn\n",
    "!{sys.executable} -m pip install imageio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "logical-vision",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pytorch imports\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "from torch import optim\n",
    "\n",
    "# numpy imports\n",
    "import numpy as np\n",
    "\n",
    "# matplotlib imports\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# sklearn imports\n",
    "from sklearn.datasets import make_regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "systematic-tactics",
   "metadata": {},
   "source": [
    "# Tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "opening-halifax",
   "metadata": {},
   "source": [
    "Torch uses tensors that are nothing more than an (multi-dimensional) array or (multi-dimensional) vector. These tensors are used for efficient execution in pytorch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "covered-boulder",
   "metadata": {},
   "source": [
    "We can easily go from lists to tensors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decimal-preview",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.Tensor([5,3])\n",
    "y = torch.Tensor([2,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "focused-receptor",
   "metadata": {},
   "source": [
    "We can do mathematical operations with our tensors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tired-investing",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x*y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "running-cookie",
   "metadata": {},
   "source": [
    "A lot of numpy like syntax can be used. For example to initialize tensors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compound-horror",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_zero = torch.zeros([2,5])\n",
    "tensor_one = torch.ones([2,5])\n",
    "tensor_random = torch.rand([2,5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "entire-wheat",
   "metadata": {},
   "source": [
    "Or to get the shape of our tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sustainable-python",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tensor_random)\n",
    "print(tensor_random.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "checked-power",
   "metadata": {},
   "source": [
    "Reshape is a bit different (pytorch uses view):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "satellite-identity",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tensor_random.shape)\n",
    "print(tensor_random.view([1,10]).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "smaller-spray",
   "metadata": {},
   "source": [
    "# Reading data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wooden-owner",
   "metadata": {},
   "source": [
    "We will use the MNIST data set that is known relatively well in the ML/DL community. THe data set contains images of written numbers (X) and the numerical value we want to predict (y):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "endangered-virtue",
   "metadata": {},
   "source": [
    "![alt text](https://upload.wikimedia.org/wikipedia/commons/2/27/MnistExamples.png \"Title\")https://en.wikipedia.org/wiki/MNIST_database"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "scheduled-immune",
   "metadata": {},
   "source": [
    "The following function is used to get the data:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aggressive-lewis",
   "metadata": {},
   "source": [
    "![alt text](figs/mnist.PNG \"Title\")https://pytorch.org/vision/0.8/datasets.html#mnist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pleasant-theme",
   "metadata": {},
   "source": [
    "Lets get the data (it is not in a tensor form yet, so we need to transform it, more on this later):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ahead-canon",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = torchvision.datasets.MNIST(\n",
    "    \"\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform = torchvision.transforms.Compose([torchvision.transforms.ToTensor()])\n",
    ")\n",
    "\n",
    "test = torchvision.datasets.MNIST(\n",
    "    \"\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform = torchvision.transforms.Compose([torchvision.transforms.ToTensor()])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nuclear-favorite",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vulnerable-handbook",
   "metadata": {},
   "source": [
    "Prepare the MNIST object for training. We need to indicate a batch size and we can shuffle the input data:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "systematic-disposition",
   "metadata": {},
   "source": [
    "![alt text](figs/dataloader.PNG \"Title\")https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "greek-bridge",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = torch.utils.data.DataLoader(\n",
    "    train,\n",
    "    batch_size=10,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "testset = torch.utils.data.DataLoader(\n",
    "    test,\n",
    "    batch_size=10,\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "surprising-abraham",
   "metadata": {},
   "source": [
    "The generated train set is a DataLoader object we can iterate over:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adverse-afternoon",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(trainset)\n",
    "print(dir(trainset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "challenging-leisure",
   "metadata": {},
   "source": [
    "Each iteration we can get a data batch that contains our X and y, and these objects (just like numpy) can be sliced:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wound-struggle",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for data in trainset:\n",
    "    print(\"Batch:\")\n",
    "    print(data)\n",
    "    print(\"Data point:\")\n",
    "    print(data[0][0])\n",
    "    print(\"Shape:\")\n",
    "    print(data[0][0].shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fitted-storm",
   "metadata": {},
   "source": [
    "Lets take a point to plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "noticed-perception",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_i,y_i = data[0][0], data[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "contemporary-postage",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_i,y_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "annual-medicine",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(x_i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dependent-canal",
   "metadata": {},
   "source": [
    "![alt text](https://i.redd.it/gwuw7zd01gd31.jpg \"Title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "secure-ensemble",
   "metadata": {},
   "source": [
    "We need to transform so we lose the \"1\" dimension and we have a matrix of 28x28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "julian-turkey",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_i)\n",
    "plt.imshow(x_i.view(28,28))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "constitutional-mathematics",
   "metadata": {},
   "source": [
    "# Build a neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "opposite-torture",
   "metadata": {},
   "source": [
    "Now that we have the data lets build our network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "skilled-education",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define class, inherits from \"nn.Module\"\n",
    "class Net(nn.Module):\n",
    "    # specify initializitation instructions\n",
    "    def __init__(self):\n",
    "        # run initialization from \"nn.Module\"\n",
    "        super().__init__()\n",
    "        \n",
    "        # define the fully connected layers\n",
    "        # the \"28*28\" is the flattened dimensions of our image\n",
    "        # the output equals the amount of classes we want to predict\n",
    "        self.fc = nn.Linear(28*28, 10)\n",
    "    \n",
    "    # define forward propagation\n",
    "    def forward(self,x):\n",
    "        # input x goes through our fully connected layer\n",
    "        # the \"F.sigmoid\" defines the activation function we pass the data through\n",
    "        x = F.sigmoid(self.fc(x))\n",
    "        \n",
    "        # return the output with a softmax function\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "covered-pipeline",
   "metadata": {},
   "source": [
    "We can see the architecture here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "round-triple",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coral-vertical",
   "metadata": {},
   "source": [
    "Lets pass a random tensor throught this network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brave-financing",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_random = torch.rand([28,28])\n",
    "\n",
    "net(x_random)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "affecting-amount",
   "metadata": {},
   "source": [
    "![alt text](https://memeguy.com/photos/images/when-your-hot-friend-send-the-pic-to-you-instead-and-says-that-was-a-mistake-247792.jpg \"Title\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "occupied-oregon",
   "metadata": {},
   "source": [
    "We need to reshape or flatten the matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "altered-strip",
   "metadata": {},
   "outputs": [],
   "source": [
    "net(x_random.view(28*28))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "regional-battlefield",
   "metadata": {},
   "source": [
    "![alt text](http://memes.ucoz.com/_nw/23/92692556.jpg \"Title\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "liked-inspiration",
   "metadata": {},
   "source": [
    "Our network does not expect a single example, so we need to reshape as if we had multiple examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "infinite-forwarding",
   "metadata": {},
   "outputs": [],
   "source": [
    "net(x_random.view(1,28*28))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "removed-fleece",
   "metadata": {},
   "source": [
    "![alt text](https://gray-wilx-prod.cdn.arcpublishing.com/resizer/TNIRzaTbaM20cxSJYazl6zt8btM=/1200x675/smart/cloudfront-us-east-1.images.arcpublishing.com/gray/X7CY2J6TIJNB7N4IAIDYKPGKFM.jpg \"Title\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gross-comfort",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "variable-costs",
   "metadata": {},
   "source": [
    "Define the optimizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cooked-major",
   "metadata": {},
   "outputs": [],
   "source": [
    "# say what should be optimized (net.parameters()) and set a learning rate (=lr)\n",
    "optimizer = optim.Adam(net.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lightweight-sequence",
   "metadata": {},
   "source": [
    "Lets get training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tribal-projector",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define number of passes we want to go through our data\n",
    "num_epochs = 3\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for data in trainset:\n",
    "        # get the tensors we will use\n",
    "        X, y = data\n",
    "        \n",
    "        # set gradients to zero, so that batches are seperated and we do not \n",
    "        # continue gradient calculations of the previous batch\n",
    "        net.zero_grad()\n",
    "        \n",
    "        # as before make sure we put in the right dimensions\n",
    "        output = net(X.view(-1, 28*28))\n",
    "        \n",
    "        # calculate the (negative log) loss based on output predictions and actual values y\n",
    "        loss = F.nll_loss(output, y)\n",
    "        \n",
    "        # a magic function that does backpropagation\n",
    "        loss.backward()\n",
    "        \n",
    "        # update the network parameters\n",
    "        optimizer.step()\n",
    "    \n",
    "    print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "agricultural-click",
   "metadata": {},
   "source": [
    "# Build a multilayered neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "previous-religious",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define class, inherits from \"nn.Module\"\n",
    "class Net(nn.Module):\n",
    "    # specify initializitation instructions\n",
    "    def __init__(self):\n",
    "        # run initialization from \"nn.Module\"\n",
    "        super().__init__()\n",
    "        \n",
    "        # define the fully connected layers\n",
    "        # the \"28*28\" is the flattened dimensions of our image\n",
    "        # the output equals the amount of classes we want to predict\n",
    "        self.fc1 = nn.Linear(28*28, 64)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.fc3 = nn.Linear(64, 64)\n",
    "        self.fc4 = nn.Linear(64, 10)\n",
    "    \n",
    "    # define forward propagation\n",
    "    def forward(self,x):\n",
    "        # input x goes through our fully connected layer\n",
    "        # the \"F.sigmoid\" defines the activation function we pass the data through\n",
    "        x = F.relu(self.fc1(x))\n",
    "        \n",
    "        # note how we change X to be the activation function of layer 1 and use this as an\n",
    "        # input here\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.sigmoid(self.fc4(x))\n",
    "        \n",
    "        # return the output with a softmax function\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "humanitarian-spending",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "august-pattern",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(net.parameters(), lr=1e-3)\n",
    "\n",
    "# define number of passes we want to go through our data\n",
    "num_epochs = 3\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for data in trainset:\n",
    "        # get the tensors we will use\n",
    "        X, y = data\n",
    "        \n",
    "        # set gradients to zero, so that batches are seperated and we do not \n",
    "        # continue gradient calculations of the previous batch\n",
    "        net.zero_grad()\n",
    "        \n",
    "        # as before make sure we put in the right dimensions\n",
    "        output = net(X.view(-1, 28*28))\n",
    "        \n",
    "        # calculate the (negative log) loss based on output predictions and actual values y\n",
    "        loss = F.nll_loss(output, y)\n",
    "        \n",
    "        # a magic function that does backpropagation\n",
    "        loss.backward()\n",
    "        \n",
    "        # update the network parameters\n",
    "        optimizer.step()\n",
    "    \n",
    "    print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "committed-sussex",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "# make sure we do not optimize any gradients\n",
    "with torch.no_grad():\n",
    "    for data in trainset:\n",
    "        X, y = data\n",
    "        output = net(X.view(-1,28*28))\n",
    "        for idx, i in enumerate(output):\n",
    "            if torch.argmax(i) == y[idx]:\n",
    "                correct += 1\n",
    "            total += 1\n",
    "print(\"Correctly assigned:\",correct,\"out of\",total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gorgeous-perry",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "# make sure we do not optimize any gradients\n",
    "with torch.no_grad():\n",
    "    for data in testset:\n",
    "        X, y = data\n",
    "        output = net(X.view(-1,28*28))\n",
    "        for idx, i in enumerate(output):\n",
    "            if torch.argmax(i) == y[idx]:\n",
    "                correct += 1\n",
    "            total += 1\n",
    "print(\"Correctly assigned:\",correct,\"out of\",total)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "announced-module",
   "metadata": {},
   "source": [
    "# Apply some regularization with dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dressed-sample",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define class, inherits from \"nn.Module\"\n",
    "class Net(nn.Module):\n",
    "    # specify initializitation instructions\n",
    "    def __init__(self):\n",
    "        # run initialization from \"nn.Module\"\n",
    "        super().__init__()\n",
    "        \n",
    "        # define the fully connected layers\n",
    "        # the \"28*28\" is the flattened dimensions of our image\n",
    "        # the output equals the amount of classes we want to predict\n",
    "        self.fc1 = nn.Linear(28*28, 64)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.dropout = nn.Dropout(0.25)\n",
    "        self.fc3 = nn.Linear(64, 64)\n",
    "        self.fc4 = nn.Linear(64, 10)\n",
    "        \n",
    "    \n",
    "    # define forward propagation\n",
    "    def forward(self,x):\n",
    "        # input x goes through our fully connected layer\n",
    "        # the \"F.sigmoid\" defines the activation function we pass the data through\n",
    "        x = F.relu(self.fc1(x))\n",
    "        \n",
    "        # note how we change X to be the activation function of layer 1 and use this as an\n",
    "        # input here\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.relu(self.fc4(x))\n",
    "        \n",
    "        # return the output with a softmax function\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gothic-loading",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "noted-chosen",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(net.parameters(), lr=1e-3)\n",
    "\n",
    "# define number of passes we want to go through our data\n",
    "num_epochs = 3\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for data in trainset:\n",
    "        # get the tensors we will use\n",
    "        X, y = data\n",
    "        \n",
    "        # set gradients to zero, so that batches are seperated and we do not \n",
    "        # continue gradient calculations of the previous batch\n",
    "        net.zero_grad()\n",
    "        \n",
    "        # as before make sure we put in the right dimensions\n",
    "        output = net(X.view(-1, 28*28))\n",
    "        \n",
    "        # calculate the (negative log) loss based on output predictions and actual values y\n",
    "        loss = F.nll_loss(output, y)\n",
    "        \n",
    "        # a magic function that does backpropagation\n",
    "        loss.backward()\n",
    "        \n",
    "        # update the network parameters\n",
    "        optimizer.step()\n",
    "    \n",
    "    print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "traditional-neighbor",
   "metadata": {},
   "source": [
    "# Regression example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "meaning-stephen",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_sklearn, y_sklearn = make_regression(n_samples=500, n_features=1, bias=2, noise=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "designing-transaction",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.from_numpy(abs(X_sklearn)).float()\n",
    "y = torch.from_numpy(abs(y_sklearn)/100).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "educated-smith",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.layer = torch.nn.Linear(1, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer(x)      \n",
    "        return x\n",
    "\n",
    "net = Net()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "portuguese-nigeria",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "net = Net()\n",
    "num_epochs = 25\n",
    "batch_size = 50\n",
    "optimizer = optim.SGD(net.parameters(), lr=1e-2)\n",
    "loss_func = torch.nn.MSELoss()\n",
    "fig, ax = plt.subplots(figsize=(10,10))\n",
    "\n",
    "# create batches\n",
    "trainset = torch.utils.data.DataLoader(\n",
    "    # create a tensor data set!\n",
    "    TensorDataset(X,y),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for data in trainset:\n",
    "        X_batch, y_batch = data\n",
    "        \n",
    "        # set gradients to zero, so that batches are seperated and we do not \n",
    "        # continue gradient calculations of the previous batch\n",
    "        net.zero_grad()\n",
    "        \n",
    "        prediction = net(X_batch)\n",
    "        loss = loss_func(prediction.view(batch_size), y_batch)\n",
    "        \n",
    "        print(\"Batch loss: \",loss)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "plt.scatter(X_batch.data.numpy(), y_batch.data.numpy(), color = \"orange\")\n",
    "plt.plot(X_batch.data.numpy(), prediction.data.numpy(), 'g-', lw=3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adjacent-jewel",
   "metadata": {},
   "source": [
    "# GPU learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aerial-confusion",
   "metadata": {},
   "source": [
    "Up till now everything was run on the CPU, for larger and more complex data sets we should use the GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "impossible-headline",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "executed-placement",
   "metadata": {},
   "source": [
    "Select the specific GPU device (you can also list devices available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "invalid-necklace",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "confirmed-newton",
   "metadata": {},
   "source": [
    "Test if we should run on the GPU or CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afraid-holly",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    print(\"Using the GPU\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Using the CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "speaking-citation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define class, inherits from \"nn.Module\"\n",
    "class Net(nn.Module):\n",
    "    # specify initializitation instructions\n",
    "    def __init__(self):\n",
    "        # run initialization from \"nn.Module\"\n",
    "        super().__init__()\n",
    "        \n",
    "        # define the fully connected layers\n",
    "        # the \"28*28\" is the flattened dimensions of our image\n",
    "        # the output equals the amount of classes we want to predict\n",
    "        self.fc1 = nn.Linear(28*28, 64)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.dropout = nn.Dropout(0.25)\n",
    "        self.fc3 = nn.Linear(64, 64)\n",
    "        self.fc4 = nn.Linear(64, 10)\n",
    "        \n",
    "    \n",
    "    # define forward propagation\n",
    "    def forward(self,x):\n",
    "        # input x goes through our fully connected layer\n",
    "        # the \"F.sigmoid\" defines the activation function we pass the data through\n",
    "        x = F.relu(self.fc1(x))\n",
    "        \n",
    "        # note how we change X to be the activation function of layer 1 and use this as an\n",
    "        # input here\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.relu(self.fc4(x))\n",
    "        \n",
    "        # return the output with a softmax function\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collected-investigator",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brave-peace",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = torchvision.datasets.MNIST(\n",
    "    \"\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform = torchvision.transforms.Compose([torchvision.transforms.ToTensor()])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rotary-hygiene",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = torch.utils.data.DataLoader(\n",
    "    train,\n",
    "    batch_size=10,\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "distributed-elizabeth",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(net.parameters(), lr=1e-3)\n",
    "\n",
    "# define number of passes we want to go through our data\n",
    "num_epochs = 3\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for data in trainset:\n",
    "        # get the tensors we will use\n",
    "        X, y = data\n",
    "        \n",
    "        # set gradients to zero, so that batches are seperated and we do not \n",
    "        # continue gradient calculations of the previous batch\n",
    "        net.zero_grad()\n",
    "        \n",
    "        # as before make sure we put in the right dimensions\n",
    "        output = net(X.view(-1, 28*28))\n",
    "        \n",
    "        # calculate the (negative log) loss based on output predictions and actual values y\n",
    "        loss = F.nll_loss(output, y)\n",
    "        \n",
    "        # a magic function that does backpropagation\n",
    "        loss.backward()\n",
    "        \n",
    "        # update the network parameters\n",
    "        optimizer.step()\n",
    "    \n",
    "    print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "furnished-smile",
   "metadata": {},
   "source": [
    "Everything needs to be on the GPU..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "homeless-norwegian",
   "metadata": {},
   "source": [
    "![alt text](https://media1.tenor.com/images/48290b257ad7e7fa0a832f525b39818c/tenor.gif?itemid=13782932 \"Title\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "certain-engineering",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(net.parameters(), lr=1e-3)\n",
    "\n",
    "# define number of passes we want to go through our data\n",
    "num_epochs = 3\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for data in trainset:\n",
    "        # get the tensors we will use\n",
    "        X, y = data\n",
    "        \n",
    "        # set gradients to zero, so that batches are seperated and we do not \n",
    "        # continue gradient calculations of the previous batch\n",
    "        net.zero_grad()\n",
    "        \n",
    "        # as before make sure we put in the right dimensions\n",
    "        output = net(X.view(-1, 28*28).to(device))\n",
    "        \n",
    "        # calculate the (negative log) loss based on output predictions and actual values y\n",
    "        loss = F.nll_loss(output, y.to(device))\n",
    "        \n",
    "        # a magic function that does backpropagation\n",
    "        loss.backward()\n",
    "        \n",
    "        # update the network parameters\n",
    "        optimizer.step()\n",
    "    \n",
    "    print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tender-consideration",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
